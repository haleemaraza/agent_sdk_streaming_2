{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiSICLEK9CzgqWefklQ90d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haleemaraza/agent_sdk_streaming_2/blob/main/stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlHhNtSvjWJ0",
        "outputId": "982be448-0a61-4b6e-b424-85c3215675f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m735.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.1/166.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "8838mXjGlzSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled\n",
        "from google.colab import userdata\n",
        "\n",
        "google_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not google_api_key:\n",
        "  raise ValueError(\"GGOGLE_API_KEY is not set. Please ensure it is defined in your .env file.\")\n",
        "\n",
        "external_client=AsyncOpenAI(\n",
        "  api_key = google_api_key,\n",
        "  base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "set_tracing_disabled(disabled=True)"
      ],
      "metadata": {
        "id": "e-BdxgPNl5Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def main_1():\n",
        "    agent=Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"you are a helpful assistance.\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"please tell me 5 jokes.\")\n",
        "\n",
        "    async for event in result.stream_events():\n",
        "      print(event)\n",
        "\n",
        "asyncio.run(main_1())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzyvNmajEF8v",
        "outputId": "57890288-f000-44d1-ad48-947810d51a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='you are a helpful assistance.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7eb9cec23d10>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1755083659.4858303, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Alright', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\", here are 5 jokes for you:\\n\\n1.  Why don'\", item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='t scientists trust atoms?\\n    Because they make up everything!\\n\\n2.', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n3.  What do you call a lazy kangaroo?\\n    ', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Pouch potato!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What musical instrument is found', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=7, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' in the bathroom?\\n    A tuba toothpaste!\\n', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=8, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n3.  What do you call a lazy kangaroo?\\n    Pouch potato!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What musical instrument is found in the bathroom?\\n    A tuba toothpaste!\\n\", type='output_text', logprobs=None), sequence_number=9, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n3.  What do you call a lazy kangaroo?\\n    Pouch potato!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What musical instrument is found in the bathroom?\\n    A tuba toothpaste!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=10, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1755083659.4858303, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n3.  What do you call a lazy kangaroo?\\n    Pouch potato!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What musical instrument is found in the bathroom?\\n    A tuba toothpaste!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=11, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='you are a helpful assistance.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7eb9cec23d10>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n3.  What do you call a lazy kangaroo?\\n    Pouch potato!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What musical instrument is found in the bathroom?\\n    A tuba toothpaste!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent,Runner\n",
        "\n",
        "async def main"
      ],
      "metadata": {
        "id": "qWs9k8lJs6_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def main_1():\n",
        "    agent=Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"you are a helpful assistance.\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"please tell me 5 jokes.\")\n",
        "\n",
        "    async for event in result.stream_events():\n",
        "      print(event)\n",
        "\n",
        "asyncio.run(main_1())"
      ],
      "metadata": {
        "id": "zB1yVXwtbTKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}